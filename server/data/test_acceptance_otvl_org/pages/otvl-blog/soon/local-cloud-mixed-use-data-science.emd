<div otvl-web>
type: sf_q_img_in_card
src: /assets/images/local-cloud-mixed-use-data-science/versEtangDeSoulcem.jpg
alt: Article image
title: Vers l'Ã©tang de Soulcem
</div>

# Mixing local and cloud data access for Data Science

![logo work in progess](/assets/images/common/wip.png "Logo work in progress")

** Preview: ** this is a work in progress.

## Introduction

Relying on Cloud Computing services is often an opportunity for Data Science projects,
however, as both local and cloud processing must often coexist,
the data access must be designed carefully.

After browsing available solutions for the support of various use cases,
this article discusses their implementations.

You will find useful references at the bottom of this page.

## Use cases and organisational constraints

### Data Science use cases and the Data Lake

There is a wide range of Data Science types of use cases,
and as some of them require the fast processing of large datasets
the assumption is often made that a Big Data platform is required.
While this is generally true concerning Data Analytics projects,
most of the Data Science applications do not require such

### Going beyond the Data Lake 

As we can stay too easily focused on solutions built around existing Data Lake products,
it is important to keep requirements and solutions as separate as possible.
I found the article
_"[How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)"_
 very enlightening concerning this question, the two main challenges that it describes being:

- the increasing number of data sources, along with their various organisation origins,
- and the increasing number of innovation use cases requiring specific data processing.

The article also explains how existing platforms shape the architecture of the solutions
around the data processing pipeline, making it difficult to integrate new data sources
as those pipelines are monolithic with respect to the organization.

Concerning the _Move to a Distributed Data Mesh_
 I let you read through this excellent article if you didn't already know it.
Some important consequences of implementing such an architecture are:

- that a reusable dataset scope is no more restricted to raw data like it was in a Data Lake,
  and it can now include the result of any intermediate processing of this data 
  if it is relevant for cross-domain reuse;
- it can even be provided through advanced data services such as ones found for instance
  with graph or time-series databases;
- that those datasets have to be considered as products, reusing the existing know-how of building
  any other software product;
- that the data processing pipelines are reshaped and this time aligned with the domain boundaries,
- those datasets product services being naturally provided by their owner domains.

### Domain driven architecture

We could represent such data services composition opportunities on a schema like the one below
(the arrows come from the consumer to the provider).
Each domain is represented by a colour, providing data services, built upon data engineering tools,
and that can be organized in pipelines.
We create thus a mesh of data services the same way we do for other kind of services. 

<div otvl-web>
type: sf_q_img_in_card
src: /assets/images/local-cloud-mixed-use-data-science/local_cloud_mixed_use_ds_mesh.jpg
alt: Domain Data Services Mesh schema
title: Domain Data Services Mesh
class_: self_width_img
</div>

The data services gouvernance is clear and enables cross-domain reuse when wanted,
as soon as service level agreements are defined.
The potential cross-domain reuse of this resulting _"domain data as a product"_
makes our initial assumption, mixing local and cloud data access, even more likely.

To conlude on the functional point of view, we can see that this move
towards a distributed data mesh provides all the organisational agility
required for answering new challenges concerning Data Science,
while taking benefit of know-how and best practices from other technical domains.

We will consider the implementation questions in the following sections,
as this is the domain where Data Science has very specific requirements.
The services and pipelines implementation responsibilities are naturally defined
with the same domain boundaries as the services they provide,
and the architecture does not forbid technical solutions reuse,
while not making it mandatory.

## Solutions and operational constraints

## Implementations

## Conclusion and further perspective

## References

**Articles and presentations**

- [Continuous Delivery for Machine Learning](https://martinfowler.com/articles/cd4ml.html)
- [How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)
- [The curse of the data lake monster](https://www.thoughtworks.com/insights/blog/curse-data-lake-monster)
- [Modern Data Lake with Minio : Part 1](https://blog.minio.io/modern-data-lake-with-minio-part-1-716a49499533)
- [Next-Gen Data Lakes and Analytics Platforms](https://www.slideshare.net/AmazonWebServices/nextgen-data-lakes-and-analytics-platforms-aws-summit-sydney)
- [Everything a Data Scientist Should Know About Data Management](https://www.kdnuggets.com/2019/10/data-scientist-data-management.html)

**Reference materials**

- [martinFowler - DataLake](https://martinfowler.com/bliki/DataLake.html)

** Security related **

- [Amazon Cognito](https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html)
- [OpenStack Swift ACLs](https://docs.openstack.org/swift/latest/overview_acl.html)
