<div otvl-web>
type: sf_q_img_in_card
src: /assets/images/local-cloud-mixed-use-data-science/versEtangDeSoulcem.jpg
alt: Article image
title: Vers l'Ã©tang de Soulcem
</div>

# Mixing local and cloud data access for Data Science

![logo work in progess](/assets/images/common/wip.png "Logo work in progress")

** Preview: ** this is a work in progress.

## Introduction

Relying on Cloud Computing services is often an opportunity for Data Science projects,
however, as both local and cloud processing must often coexist,
the data access must be designed carefully.

After browsing available solutions for the support of various use cases,
this article discusses their implementations.

It may be useful to consider first
that there is a wide range of requirement types as far as a Big Data Platform is concerned.
You may browse the presentation from AWS 
_"[Next-Gen Data Lakes and Analytics Platforms](https://www.slideshare.net/AmazonWebServices/nextgen-data-lakes-and-analytics-platforms-aws-summit-sydney)"_
to make your opinion.
To keep things simple, we will restrict the discussion here
to low-end and medium-range performance requirements concerning what could be called
the _data access bandwidth_, i.e. the need to access a large dataset in a small time frame.
However we will give a perspective on high-end performance requirements in the last part of the article.

You will find useful references at the bottom of this page.

## Use cases and organisational constraints

As we can too easily stay focused on solutions built around existing datalake products,
it is important to keep requirements and solutions as separate as possible.
I found the article
_"[How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)"_
 very enlightening concerning this question, the two main challenges being:

- the increasing number of data sources, along with their various organisation origins,
- and the increasing number of innovation use cases requiring specific data processing.

The mentioned article also explains how existing platforms shape the architecture of the solutions
around the data processing pipeline, making it difficult to integrate new data sources
as those pipelines are monolithic with respect to the organization.

Concerning the _"Move to a Distributed Data Mesh"_
 I let you read through this excellent article if you didn't already know it.
Some important consequences of implementing such an architecture are:

- that a reusable dataset scope is no more restricted to raw data like it was in a datalake,
  and it can now include the result of any intermediate processing of this data 
  if it is relevant for cross-domain reuse;
- that the data processing pipelines are reshaped by the domain boudaries;
- that those datasets have to be considered as products, reusing the existing know-how of building
  any other software product.

The potential cross-domain reuse of this resulting _"domain data as a product"_
makes the occurence of our initial assumption, mixing local and cloud access,
even more likely.

## Solutions and operational constraints

## Implementations

## Conclusion

## References

**Articles and presentations**

- [Continuous Delivery for Machine Learning](https://martinfowler.com/articles/cd4ml.html)
- [How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)
- [The curse of the data lake monster](https://www.thoughtworks.com/insights/blog/curse-data-lake-monster)
- [Modern Data Lake with Minio : Part 1](https://blog.minio.io/modern-data-lake-with-minio-part-1-716a49499533)
- [Next-Gen Data Lakes and Analytics Platforms](https://www.slideshare.net/AmazonWebServices/nextgen-data-lakes-and-analytics-platforms-aws-summit-sydney)

**Reference materials**

- [martinFowler - DataLake](https://martinfowler.com/bliki/DataLake.html)
